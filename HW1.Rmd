---
title: "STA521 HW1"
author: "Emre Yurtbay // eay7"
date: "Due August 28"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
# add other libraries here

knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(dplyr)
library(ggplot2)
library(GGally)
```

This exercise involves the Auto data set from ISLR.  Load the data and answer the following questions adding your code in the code chunks. Please submit a pdf version to Sakai.  For full credit, you should push your final Rmd file to your github repo on the STA521-F19 organization site by the deadline  (the version that is submitted on Sakai will be graded)

```{r data, echo=F}
data(Auto)
```

## Exploratory Data Analysis
### 1. Create a summary of the data.  How many variables have missing data?

```{r}
summary(Auto)
```
```{r}
count_vars_with_NA <- 0
for (name in names(Auto)) {
  col <- Auto$name
  if (any(is.na(col))) {
    count_vars_with_NA = count_vars_with_NA + 1
  }
}
count_vars_with_NA
```

None of the variables have missing data.

```{r}
help(Auto)
```

We are told from the help file that the original dataset had some missing values, but the one we are using right now does not.

### 2.  Which of the predictors are quantitative, and which are qualitative?
```{r}
sapply(Auto, class) %>% knitr::kable()
```

`mpg`, `cylinders`, `displacement`, `horsepower`, `weight`, `acceleration`, and `year` are
all quantitative variables. Note that `year` and `cylinders` are numerical varaibles that only take on certain discrete values, meaning one is justified in classifying them as qualitative as well. `origin` gives the country in which the car came from, making it  a qualitative variable, even though it is coded numerically, with values 1, 2, and 3. If we are going to use it in a model, we should consider converting it into a factor. `Name` is a qualitative variable.


### 3. What is the range of each quantitative predictor? You can answer this using the `range()` function.   Create a table with variable name, min, max with one row per variable.   `kable` from the package `knitr` can display tables nicely.

```{r}
range_summary <- sapply(Auto %>% 
                          select(
                            `mpg`, `cylinders`,
                            `displacement`, `horsepower`, 
                            `weight`, `acceleration`), range) %>%
  t()
colnames(range_summary) <- c("Min", "Max")
range_summary %>% knitr::kable()
```

### 4. What is the mean and standard deviation of each quantitative predictor?  _Format nicely in a table as above_

```{r}
mean_sd_function <- function(x){c(mean(x), sd(x))}
sum_stats <- sapply(Auto %>% select(`mpg`, 
                                    `cylinders`, `displacement`, 
                                    `horsepower`, `weight`, `acceleration`),
                    mean_sd_function) %>% t()
colnames(sum_stats) <- c("Mean", "Standard Deviation")
sum_stats %>% knitr::kable()
```


### 5. Investigate the predictors graphically, using scatterplot matrices  (`ggpairs`) and other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.  _Try adding a caption to your figure_
```{r}
Auto_Quantitative <- Auto %>% select(-c(`name`))
m <- GGally::ggpairs(Auto_Quantitative)
m <- m + 
  labs(subtitle ="Matrix of Scatterplots for the Auto Data",
              caption = "Caption: Use this to check for linear relationships among data")
m <- m + 
  theme(plot.subtitle=element_text(size=12, hjust=0.5, face="italic", color="black"))
m
```

Using the pairplot, we can investigate the relationship between multiple variables in the data set to determine whether or not they are appropriate to consider for linear regression. On the diagonals, we can see the densities for each variable in the dataset. It appears that many of the distributions of our variables are right skewed, including mpg, dispalcement, horsepower, and weight. If we zoom in on the density plot for `mpg`, we can more easily see this right skewness.

```{r}
ggplot(data = Auto, mapping = aes(x = `mpg`)) + geom_density(col = "red")
```


Acceleration is approximately mound shaped, and year is approximately uniform. We can also examine relationships between multiple variables. Displacement and horsepower have a positive linear relationship, as do dispalcement and weight. Horsepower and acceleration have a negative linear relationship, as do horsepower and weight. Here is a plot of horsepower vs. weight, which showcases the positive linear relationship

```{r}
ggplot(data = Auto, mapping = aes(x = `horsepower`, y = `weight`)) + geom_point()
```



Mpg has a negative non-linear relationship with displacement, horsepower, and weight. Cars with more cylinders also seem to have lower mpg, while European and Japanese cars tend to have higher mpg values than American cars. Note that (1) stands for American, (2) stands for European, and (3) stands for Japanese.

```{r}
ggplot(data = Auto, mapping = aes(x = as.factor(`origin`), y = `mpg`)) + geom_boxplot()
```


### 6. Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables using regression. Do your plots suggest that any of the other variables might be useful in predicting mpg using linear regression? Justify your answer.

If we want to perform linear regression to predict mpg, we expect that mpg and the variables that we use to predict it have an approximately linear relationship. So let us examine a plot showing the relationship between `mpg` and `displacement`

```{r}
ggplot(data = Auto, mapping = aes(x = `displacement`, y = `mpg`)) + geom_point()
```

While there is a clear relationship between the two variables, the relationship appears to be non-linear, so a simple regression may not be an appropriate model. There is a similar curved relationship between `mpg` and `horsepower`, as well as between `mpg` and `weight`. A polynomial regression may be more appropriate for modelling these relationships. Another thing we could do is perform a log transform to these 3 variables, making them have a more linear relationship with `mpg`.

```{r}
ggplot(data = Auto, mapping = aes(x = log(`displacement`), y = `mpg`)) + geom_point()
```

As we can see, the log transform of the displacement variable seems to have made the relationship seem more linear, and now linear regression may be appropriate.

```{r}
ggplot(data = Auto, mapping = aes(x = `acceleration`, y = `mpg`)) + 
  geom_point()
```

`mpg` and `accerleration` may have a fairly weak linear relationship, so we may be able to use `acceleration` to model `mpg`, but the model may not be entirely useful.

```{r}
ggplot(data = Auto, mapping = aes(x = `year`, y = `mpg`)) + geom_point()
```

It appears that as `year` increases, `mpg` also increases in a fairly linear way, so `year` may be an appropriate predictor for `mpg` in a linear regression context. Similarly, there seems to be a weak negative linear relationship between `mpg` and `cylinders`, so that may be another useful predictor. Based on the scatterplot matrix, there may also be a possible relationship between a car's `origin` and its `mpg`

## Simple Linear Regression

7.  Use the `lm()` function to perform a simple linear 
regression with `mpg` as the response and `horsepower` as the
predictor. Use the `summary()` function to print the results.
Comment on the output.
For example:
    (a) Is there a relationship between the predictor and the response?
    (b) How strong is the relationship between the predictor and
the response?
    (c) Is the relationship between the predictor and the response
positive or negative?
    (d)  Provide a brief interpretation of the parameters that would suitable for discussing with a car dealer, who has little statistical background.
    (e) What is the predicted mpg associated with a horsepower of
98? What are the associated 95% confidence and prediction
intervals?   (see `help(predict)`) Provide interpretations of these for the car dealer.



```{r}
reg <- lm(mpg~horsepower, data = Auto)
summary(reg)
```

There is a statisitcally signficant moderately strong ($R^2 = 0.6$) negative relationship between horsepower and miles per gallon, as the p-value associated with the slope is approximately 0. That is, as horsepower increases, a car's mpg value will decrease. A unit increase in the car's horsepower corresponds to a 0.16 unit decrease in the car's miles per gallon, while a car with "0 horsepower" would theoretically have a horsepower of 39.9. One thing to note may be that a plot of mpg vs. horsepower shows a curved relationship between the two variables, meaning a more flexible method, such as a higher order polynomial regression, may be more accurate than a simple linear regression model. We could also use a log transform on the predictor to solve this problem.

```{r}
# Predicted mpg with horsepower 98
new = data.frame(horsepower=98)
predict(reg, newdata = new)
```

```{r}
# Prediction Interval
new = data.frame(horsepower=98)
predict(reg, newdata = new, interval = "prediction", level = 0.95)
```

```{r}
# Confidence Interval
new = data.frame(horsepower=98)
predict(reg, newdata = new, interval = "confidence", level = 0.95)
```

A car with the horsepower of 98 will have a predicted mpg of 24.467.

The confidence interval is (23.97308 24.96108) This means that reasonable values for the expectation of $y|x$ lie in the range (23.97308 24.96108), with 95% certainty. The prediction interval is (14.8094, 34.12476), which means  reasonable values for the predicted value y at point x lie in the range (14.8094, 34.12476), with 95% certainty.

The interpretation for the confidence interval is slightly different from the interpretation of the prediction interval. The confidence interval gives a range of values for the  *expected value* of the predicted response of y given a data point x, that is, of $E[y|x]$. In simple terms, it is a confidence range for the regression line itself, and the average mpg of cars with 98 horsepower would fall between (23.97308 24.96108), with 95% certainty. As we can see, the confidence interval is quite tight compared to the prediction interval. To understand this, recall that our regression model is $y_i = f(x_i) + \epsilon_i$, where $\epsilon_i$ is error we cannot account for. The prediction interval also takes into account the extra variability of this error term, $\epsilon_i$. This variance is not taken into account by the confidence interval. Therefore, the prediction interval is necessarily wider than the confidence interval, but they are centered around the same value $x\beta$.

### 8. Plot the response and the predictor using `ggplot`. Add to the plot a line showing the least squares regression line. 
```{r}
ggplot(data = Auto, aes(x = `horsepower`, y = `mpg`))+
  geom_point()+
  geom_abline(slope =  -0.157845, intercept = 39.935861, col = "red")
```

The relationship between horsepower and mpg does not seem to be perfectly linear - there in fact seems to be some curvature in the relationship.

9. Use the `plot()` function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the model regarding assumptions for using a simple linear regression.  

```{r}
plot(reg)
```

The plot function allow us to check our modelling assumptions - if the diagnostic plots do not show the patterns we would like to see, we may assume that the linear regression model was not appropriate for our data. The first diagnostic plot we would like to examine is the "Residuals vs. Fitted" plot, which checks the assumption that there is a linear relationship between the predictor and the response. If we see a horizontal red line, and a random scatter of data points around 0, then this assumption is reasonably met.
In our data, we see a curved red line and a non random scatter of residuals, meaning the linear relationship assumption is not met. This was conjectured earlier, as there clearly was a curved relationship between the predictor and response. The next plot we see is the Normal QQ-plot, which checks to see if the residuals are normally distributed. The Normal QQ-plot should show the data points following the straight dashed line. For the most part, this is the case between theoretical quantiles -1 and 1, but we see some significant deviation in the tails, indicating that the residuals may not indeed be normally distributed. The scale location plot checks the assumption that the residuals terms have constant variance. Again, we want to see a horizontal red line with a random scatter of residuals. The scatter of the residuals terms seems to change slightly, so the residuals probably do not have perfectly constant variance, though this plot does not seem to be extremely worrying. Finally, the residuals vs. leverage plot shows us influential cases, such as outlier as leverage points, which may have a major impact on our regression line. We do see a few points with standardized residuals greater than 3 in absolute value, and 117 and 94 are marked as potential influential points. These points should be considered and investigated before being included in the model fitting process.

## Theory

#### 10) Show that the  regression function $E(Y \mid x) = f(x)$ is the optimal optimal predictor of $Y$ given $X = x$ using squared error loss:  that is $f(x)$ minimizes $E[(Y - g(x))^2 \mid X =x]$ over all functions $g(x)$ at all points $X=x$.   _Hint:  there are at least two ways to do this.   Differentiation (so think about how to justify) - or - add and subtract the proposed optimal predictor and who that it must minimize the function._

Our goal is to minimize $E_{Y\mid X}[(Y - g(x))^2 \mid X =x]$ over all functions $g(x)$ at all points $X=x$. We can write this as 

$$E_{Y\mid X}[(Y - g(x))^2 \mid X =x] = \int_{-\infty}^{\infty}(Y - g(x))^2 f_{Y\mid X}(y\mid x)dy $$
Expanding out the squared term inside the integral, we have the following 

$$E_{Y\mid X}[(Y - g(x))^2 \mid X =x] =\int_{-\infty}^{\infty}(Y - g(x))^2 f_{Y\mid X}(y\mid x)dy = \int_{-\infty}^{\infty}(y^2 -2g(x)y + g(x)^2) f_{Y\mid X}(y\mid x)dy$$
This can be split into three integrals, 

$$E_{Y\mid X}[(Y - g(x))^2 \mid X =x] = \int_{-\infty}^{\infty}y^2f_{Y\mid X}(y\mid x)dy -2g(x) \int_{-\infty}^{\infty}yf_{Y\mid X}(y\mid x)dy + g(x)^2\int_{-\infty}^{\infty}f_{Y\mid X}(y\mid x)dy$$

We can simplify this into a more friendly looking form

$$E_{Y\mid X}[(Y - g(x))^2 \mid X =x] =E_{Y\mid X}[Y^2 \mid X =x] + 2g(x)E_{Y\mid X}[Y \mid X =x] + g(x)^2$$
Now, we are in the position to take the derivative with respect to g(x), as x is fixed, and to find the minimum value for $g(x)$

$$\frac{\partial}{\partial g(x)}[E_{Y\mid X}[Y^2 \mid X =x] + 2g(x)E_{Y\mid X}[Y \mid X =x] + g(x)^2] = -2E_{Y\mid X}[Y \mid X =x] + 2g(x) = 0$$
Solving for $g(x)$, we get 

$$g(x) = E[Y \mid X =x]$$

We know this is a minimum because 

$$\frac{\partial^2}{\partial g(x)^2}[E_{Y\mid X}[Y^2 \mid X =x] + 2g(x)E_{Y\mid X}[Y \mid X =x] + g(x)^2] = 2 > 0$$
Therefore, $E[Y \mid X =x]$ minimizes $E[(Y - g(x))^2 \mid X =x]$ over all functions $g(x)$ at all points $X=x$

### 11. (adopted from ELS Ex 2.7 ) Suppose that we have a sample of $N$ pairs $x_i, y_i$ drwan iid from the distribution characterized as follows 
$$ x_i \sim h(x), \text{ the design distribution}$$
$$ \epsilon_i \sim g(y), \text{ with mean 0 and variance } \sigma^2 \text{ and are independent of the } x_i $$
$$Y_i = f(x_i) + \epsilon$$

#### (a) What is the conditional expectation of $Y$ given that $X = x_o$?  ($E_{Y \mid X}[Y]$)
 
  $$E_{Y \mid X}[Y\mid X= x_o] = f(x_o)$$
  
#### (b) What is the conditional variance of $Y$ given that $X = x_o$? ($\text{Var}_{Y \mid X}[Y]$)
  
  $$\text{Var}_{Y \mid X}[Y \mid X= x_o] = \sigma^2$$
  
  
#### (c) show  that for any estimator $\hat{f}(x)$ that the conditional (given X) (expected)  Mean Squared Error can be decomposed as 
  
$$
E_{Y \mid X}[(Y - \hat{f}(x_o))^2] = \underbrace{ \text{Var}_{Y \mid X}[\hat{f}(x_o)]}_{\textit{Variance of estimator}} +
\underbrace{(f(x) - E_{Y \mid X}[\hat{f}(x_o)])^2}_{\textit{Squared Bias}} + \underbrace{\textsf{Var}(\epsilon)}_{\textit{Irreducible}}
$$

 _Hint:  try the add zero trick of adding and subtracting expected values_

First, note that
$$E_{Y \mid X}[(Y - \hat{f}(x_o))^2] = E[((Y - f(x_o)) + (f(x_0) - \hat{f}(x_o)))^2]$$
This is equivalent to 

$$E[(Y - f(x_o))^2 + (f(x_0) - \hat{f}(x_o))^2 + 2(Y - f(x_o))(f(x_0) - \hat{f}(x_o))]$$
Using linearity of expectation, we have the following
$$E[(Y - f(x_o))^2] + E[(f(x_0) - \hat{f}(x_o))^2] + 2E[(Y - f(x_o))(f(x_0) - \hat{f}(x_o))]$$
First, look at the first term, $E[(Y - f(x_o))^2]$. This can be rewritten as the following
$$E[(Y - f(x_o))^2] = E[(f(x_o) + \epsilon -f(x_o))^2] = E[\epsilon^2]$$
Note that $Var(\epsilon) = E[\epsilon^2] - E[\epsilon]^2$. We are given that $E[\epsilon] =0$, so we know that $Var(\epsilon) = E[\epsilon^2] - 0 = E[\epsilon^2]$. Since we are also given that $Var(\epsilon) = \sigma ^2$, we can conclude

$$E[(Y - f(x_o))^2] = \sigma^2$$
Now, we examine the third term, $2E[(Y - f(x_o))(f(x_0) - \hat{f}(x_o))]$.

$$E[(Y - f(x_o))(f(x_0) - \hat{f}(x_o))] = E[Yf(x_0)] - E[Y\hat{f}(x_o)] - E[f(x_0)^2] + E[f(x_0) \hat{f}(x_o)]$$
Since $f(x_0)$ is not random, we know $E[f(x_0)^2] = f(x_0)^2$. This means the above can be simplified to 
$$E[(f(x_0)+\epsilon)f(x_0)] -E[Y\hat{f}(x_o)] - f(x_0)^2 +f(x_0)E[\hat{f}(x_o)]$$
This leads to 

$$E[f(x_0)f(x_o)] + E[\epsilon f(x_0)] - E[(f(x_0)+\epsilon)\hat{f}(x_o)]- f(x_0)^2 +f(x_0)E[\hat{f}(x_o)]$$

Another important fact is that $\epsilon$ is independent of $f(x_0)$, so we can write $E[\epsilon f(x_0)]$ as $E[\epsilon]E[f(x_0)]$. Since $E[\epsilon] = 0$, that whole term goes away. Also, since $E[f(x_0)f(x_o)] = f(x_0)^2$ We are left with

$$-f(x_0) E[\hat{f}(x_o)] - E[\epsilon \hat{f}(x_0)] + f(x_0)E[\hat{f}(x_o)] $$

The first and last two terms cancel, and the middle term is 0, as the error term is independent of the predicted value of $\hat{f}(x_0)$ at some new $x_0$ (the error term associated with $x_0$ does not have any effect on $\hat{f}(x_0)$), so we can finally conclude 

$$2E[(Y - f(x_o))(f(x_0) - \hat{f}(x_o))] =0$$

Now, we have 

$$E[(Y - f(x_o))^2] + E[(f(x_0) - \hat{f}(x_o))^2] + 2E[(Y - f(x_o))(f(x_0) - \hat{f}(x_o))] = \sigma^2 + E[(f(x_0) - \hat{f}(x_o))^2]$$

All that is left is to consider $E[(f(x_0) - \hat{f}(x_o))^2]$. First, note that since in the general case $(a-b)^2 = (b-a)^2$, we can rewrite $E[(f(x_0) - \hat{f}(x_o))^2]$ as $E[(\hat{f}(x_o)) - (f(x_0))^2]$. Now, this is where the trick of adding and subtracting variances comes into play

$$E[(\hat{f}(x_o) -f(x_0))^2] = E[((\hat{f}(x_0) - E[\hat{f}(x_o)]) + (E[\hat{f}(x_o)] + f(x_o)))^2] $$
Expanding this, we get

$$E[(\hat{f}(x_0) - E[\hat{f}(x_o)]) ^ 2] + (E[\hat{f}(x_o)] + f(x_o))^2 + 2E[(\hat{f}(x_0) - E[\hat{f}(x_o)])(E[\hat{f}(x_o)] + f(x_o))]$$
Note that the first term, $E[(\hat{f}(x_0) - E[\hat{f}(x_o)]) ^ 2]$ is exactly equivalent to the $Var[\hat{f}(x_0)]$. The second term, $(E[\hat{f}(x_o)] + f(x_o))^2$ is exactly equivalent to the squared bias of $\hat{f}(x_o)$. All that is left to show is that $2E[(\hat{f}(x_0) - E[\hat{f}(x_o)])(E[\hat{f}(x_o)] + f(x_o))] = 0$

$$E[(\hat{f}(x_0) - E[\hat{f}(x_o)])(E[\hat{f}(x_o)] + f(x_o))] = E[\hat{f}(x_o)E[\hat{f}(x_o)] - E[\hat{f}(x_o)]E[\hat{f}(x_o)] + \hat{f}(x_0)f(x_o) + f(x_o)E[\hat{f}(x_o)]$$
Now, we can push the expectation through

$$E[\hat{f}(x_0)]E[\hat{f}(x_0)]- E[\hat{f}(x_0)]E[\hat{f}(x_0)] - f(x_0)E[\hat{f}(x_0)] + f(x_0)E[\hat{f}(x_0)]$$

All these terms cancel, so we have 0. That means 

$$E[(\hat{f}(x_o) -f(x_0))^2] = E[(\hat{f}(x_0) - E[\hat{f}(x_o)]) ^ 2] + (E[\hat{f}(x_o)] + f(x_o))^2$$

Putting everything together now, we have 

$$E_{Y \mid X}[(Y - \hat{f}(x_o))^2] = E[(\hat{f}(x_0) - E[\hat{f}(x_o)]) ^ 2] + (E[\hat{f}(x_o)] - f(x_o))^2 + \sigma^2$$

Therefore, the conditional mean squared error can be decomposed into 

$$Var(\hat{f}(x_0)) + (Bias(\hat{f}(x_0)))^2 + Var(\epsilon)$$

#### (d) Explain why even if $N$ goes to infinity the above can never go to zero. e.g. even if we can learn $f(x)$ perfectly that the error in prediction will not vanish.   

In general, a good way way to simeltaneously decrease variance and bias simeltaneously is to increase the sample size $N$. We can often learn the function better this way and also decrease the uncertainty of our estimates with more data. However, no matter how much we increase the sample size, we can never decrease the irreducible error associated with $\epsilon$, meaning the lower bound on the error will be $\sigma^2$.

#### (e) Decompose the unconditional mean squared error $E_{Y, X}(f(x_o) - \hat{f}(x_o))^2$ into a squared bias and a variance component. (See ELS 2.7(c))
  
While above, we were looking at the MSE for the predictor $Y$, now we are considering the MSE of the estimator term $\hat{f}(x_o)$. The decompositions are slightly different, though related.


$$E_{Y, X}(f(x_o) - \hat{f}(x_o))^2 = E[(\hat{f}(x_o)) - f(x_o))^2]$$
We can add and subtract an expectation term, which turns out to be the critical trick.

$$E[(\hat{f}(x_o) -f(x_0))^2] = E[((\hat{f}(x_0) - E[\hat{f}(x_o)]) + (E[\hat{f}(x_o)] + f(x_o)))^2] $$

Expanding this, we get

$$E[(\hat{f}(x_0) - E[\hat{f}(x_o)]) ^ 2] + (E[\hat{f}(x_o)] + f(x_o))^2 + 2E[(\hat{f}(x_0) - E[\hat{f}(x_o)])(E[\hat{f}(x_o)] + f(x_o))]$$

Note that the first term, $E[(\hat{f}(x_0) - E[\hat{f}(x_o)]) ^ 2]$ is exactly equivalent to the $Var[\hat{f}(x_0)]$. The second term, $(E[\hat{f}(x_o)] + f(x_o))^2$ is exactly equivalent to the squared bias of $\hat{f}(x_o)$. All that is left to show is that $2E[(\hat{f}(x_0) - E[\hat{f}(x_o)])(E[\hat{f}(x_o)] + f(x_o))] = 0$

$$E[(\hat{f}(x_0) - E[\hat{f}(x_o)])(E[\hat{f}(x_o)] + f(x_o))] = E[\hat{f}(x_o)E[\hat{f}(x_o)] - E[\hat{f}(x_o)]E[\hat{f}(x_o)] + \hat{f}(x_0)f(x_o) + f(x_o)E[\hat{f}(x_o)]$$
Now, we can push the expectation through

$$E[\hat{f}(x_0)]E[\hat{f}(x_0)]- E[\hat{f}(x_0)]E[\hat{f}(x_0)] - f(x_0)E[\hat{f}(x_0)] + f(x_0)E[\hat{f}(x_0)]$$

All these terms cancel, so we have 0. Also, 

$$E[(E[\hat{f}(x_o)] - f(x_o))^2] = (E[\hat{f}(x_o)] - f(x_o))^2 = (Bias(\hat{f}(x_o)))^2$$
Finally, 

$$2E[(\hat{f}(x_o)) - E[\hat{f}(x_o)])(E[\hat{f}(x_o)] - f(x_o))] = 0$$

Therefore, 

$$E_{Y, X}[(f(x_0)- \hat{f}(x_o))^2] = E[(\hat{f}(x_0) - E[\hat{f}(x_o)]) ^ 2] + (E[\hat{f}(x_o)] - f(x_o))^2 = Var(\hat{f}(x_o)) + (Bias(\hat{f}(x_o)))^2$$


#### (f) Establish a relationship between the squared biases and variance in the above Mean squared errors.

Mean squared error is a typical way we assess model accuracy, but the key idea is that Mean squared error can be decomposed into squared bias and variance terms. Ideally, we would prefer a model that captures the realtionships seen in the training data while at the same time generalizing to test data. Therefore, we want a model that simeltaneously has low variance (stable predictions) and low bias (minimal error). However, there often exists a tradeoff between bias and variance - methods that decrease the bias of predictors often increase the variance, or vice versa. To counter this,  we can attempt to decrease both the bias and variance by increasing the size of the training set and by choosing a more apropriate model with more suitable assumptions. The two mean squared error derivations we have done, while not the same, are related. For instance, the first decomposition is 

$$E_{Y \mid X}[(Y - \hat{f}(x_o))^2] =  Var(\hat{f}(x_0)) + (Bias(\hat{f}(x_0)))^2 + Var(\epsilon)$$
Secondly, from the second derivation, we can establish that 

$$E_{Y, X}(f(x_o) - \hat{f}(x_o))^2 = E[E_{Y \mid X}[(Y - \hat{f}(x_o))^2]]$$

So the second decompostiion is the expected value of the first, but with an extra varaince term, $Var(\epsilon)$ which comes from trying to find the MSE of the predictor $Y$ rather than just the estimator $\hat{f}$.